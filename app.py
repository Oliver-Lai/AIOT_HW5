"""
AI vs Human Text Detector

A Streamlit web application that detects whether text content was generated by AI
(specifically ChatGPT) or written by a human using a pre-trained Hugging Face model.

Author: Senior Python Developer
Date: December 10, 2025
"""

import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from typing import Dict, Any
import os

# ============================================================================
# Configuration Constants
# ============================================================================
# These constants control the application's behavior and classification logic

# Classification thresholds (0.0 to 1.0 scale, displayed as percentage)
AI_THRESHOLD = 0.70        # If AI probability > 70%, classify as "AI Generated"
HUMAN_THRESHOLD = 0.70     # If Human probability > 70%, classify as "Human Written"
                           # If neither exceeds threshold, classify as "Mixed/Uncertain"

# Input constraints
MAX_CHAR_LIMIT = 5000      # Maximum characters accepted from user input
                           # Prevents excessive memory usage and processing time

# Model configuration
MODEL_NAME = "openai-community/roberta-base-openai-detector"  # HuggingFace model identifier
TASK = "text-classification"                                    # Pipeline task type
DEVICE = -1                                              # -1 for CPU, 0+ for GPU device ID
MODEL_CACHE_DIR = "./model_cache"                       # Local directory for model caching
                                                          # Reduces download time on subsequent runs


@st.cache_resource
def load_model():
    """
    Load and cache the Hugging Face model pipeline for text classification.
    
    This function uses @st.cache_resource to ensure the model is loaded only once
    and cached for subsequent requests, which is critical for Streamlit Cloud
    memory management. Models are cached locally to avoid repeated downloads.
    
    The @st.cache_resource decorator ensures:
    - Model is loaded only once per session
    - Shared across all users and reruns
    - Persists until app restart or cache clear
    - Memory-efficient for production deployment
    
    Returns:
        pipeline: Hugging Face pipeline object for text classification.
                 Returns a callable pipeline that accepts text and returns predictions.
        
    Raises:
        Exception: If model loading fails due to network issues, incompatible
                  dependencies, insufficient disk space, or invalid model name.
                  
    Note:
        First run downloads ~500MB model. Subsequent runs use cached version.
        Model files are stored in MODEL_CACHE_DIR to avoid re-downloading.
    """
    try:
        # Create cache directory if it doesn't exist
        # This ensures the model has a persistent storage location
        os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
        
        # Set the Hugging Face cache directory via environment variable
        # This tells transformers library where to store downloaded models
        os.environ['TRANSFORMERS_CACHE'] = MODEL_CACHE_DIR
        
        # Load model and tokenizer with local caching
        # The pipeline abstracts away tokenization and inference details
        classifier = pipeline(
            task=TASK,           # Type of task: text-classification
            model=MODEL_NAME,    # Pre-trained model from HuggingFace
            device=DEVICE        # CPU (-1) or GPU (0+) device
        )
        return classifier
    except Exception as e:
        # Display user-friendly error message in the UI
        st.error(f"Failed to load model: {str(e)}")
        st.info("Troubleshooting tips:\n"
                "- Check your internet connection\n"
                "- Verify the model name is correct\n"
                "- Ensure all dependencies are installed")
        # Re-raise exception to halt execution
        raise


def analyze_text(text: str, classifier) -> Dict[str, Any]:
    """
    Process text through the model and return structured results.
    
    Args:
        text (str): The input text to analyze
        classifier: The Hugging Face pipeline object
        
    Returns:
        dict: A dictionary containing:
            - ai_probability (float): Probability of AI-generated (0-100)
            - human_probability (float): Probability of human-written (0-100)
            - classification (str): "AI Generated", "Human Written", or "Mixed/Uncertain"
            - confidence_level (str): "High", "Medium", or "Low"
            
    Raises:
        Exception: If inference fails or output format is unexpected
    """
    try:
        # Truncate text if it's too long for the model (max ~512 tokens ‚âà 2000 chars)
        # This prevents indexing errors with very long texts
        MAX_MODEL_CHARS = 2000
        if len(text) > MAX_MODEL_CHARS:
            text = text[:MAX_MODEL_CHARS]
            
        # Run inference - returns list like [{'label': 'ChatGPT', 'score': 0.937}]
        # Using top_k=None to get all label scores
        results = classifier(text, top_k=None, truncation=True)
        
        # Extract probabilities
        ai_prob = 0.0
        human_prob = 0.0
        
        # results format: [[{'label': 'Real', 'score': 0.12}, {'label': 'Fake', 'score': 0.88}]]
        # Note: openai-community/roberta-base-openai-detector uses 'Fake' for AI-generated, 'Real' for human
        if isinstance(results, list) and len(results) > 0:
            scores = results[0] if isinstance(results[0], list) else results
            
            for item in scores:
                if isinstance(item, dict):
                    label = item.get('label', '').strip()
                    score = item.get('score', 0.0)
                    
                    # 'Fake' label means AI-generated
                    if label == 'Fake':
                        ai_prob = score * 100
                    # 'Real' label means human-written (real text)
                    elif label == 'Real':
                        human_prob = score * 100
        
        # Ensure probabilities sum to 100%
        if ai_prob == 0.0 and human_prob > 0.0:
            ai_prob = 100 - human_prob
        elif human_prob == 0.0 and ai_prob > 0.0:
            human_prob = 100 - ai_prob
        
        # Determine classification based on threshold comparison
        # Uses simple rule-based logic: if either probability exceeds threshold,
        # classify accordingly; otherwise mark as uncertain/mixed
        if ai_prob > (AI_THRESHOLD * 100):
            classification = "AI Generated"
        elif human_prob > (HUMAN_THRESHOLD * 100):
            classification = "Human Written"
        else:
            # Neither probability exceeds threshold - model is uncertain
            classification = "Mixed/Uncertain"
        
        # Determine confidence level based on maximum probability
        # Higher max probability = more confident prediction
        # This is independent of the classification result
        max_prob = max(ai_prob, human_prob)
        if max_prob > 85:
            confidence_level = "High"      # Very confident (>85%)
        elif max_prob >= 70:
            confidence_level = "Medium"    # Moderately confident (70-85%)
        else:
            confidence_level = "Low"       # Low confidence (<70%)
        
        return {
            'ai_probability': ai_prob,
            'human_probability': human_prob,
            'classification': classification,
            'confidence_level': confidence_level
        }
        
    except Exception as e:
        st.error(f"Error during text analysis: {str(e)}")
        raise


def main():
    """
    Main application function that sets up the Streamlit UI and handles user interactions.
    
    This function orchestrates the entire application flow:
    1. Configure page settings (title, icon, layout)
    2. Display header and model information
    3. Load and cache the ML model
    4. Create input interface for text entry
    5. Handle analysis button click
    6. Validate user input
    7. Perform text analysis
    8. Display results with visualizations
    9. Provide interpretation guidance
    
    The function uses Streamlit's reactive programming model where the entire
    script reruns on each user interaction. State is preserved using caching.
    
    Returns:
        None: This function manages the UI and doesn't return a value.
    """
    # Page configuration - must be the first Streamlit command
    # Sets browser tab title, favicon, and page width
    st.set_page_config(
        page_title="AI Content Detector",  # Browser tab title
        page_icon="üîç",                     # Browser tab icon
        layout="centered"                   # Content width: centered or wide
    )
    
    # Header Section
    st.title("üîç AI Content Detector")
    st.markdown("""
    This tool helps you detect whether text was generated by AI (ChatGPT) or written by a human.
    Simply paste your text below and click **Analyze Text** to see the results.
    """)
    
    st.info(f"**Model**: {MODEL_NAME} (trained to detect AI-generated text)")
    
    # Load model
    try:
        with st.spinner("Loading AI detection model..."):
            classifier = load_model()
    except Exception:
        st.stop()
    
    # Input Section
    st.markdown("---")
    st.subheader("üìù Enter Text to Analyze")
    
    # Initialize session state for clear counter
    if 'clear_counter' not in st.session_state:
        st.session_state.clear_counter = 0
    
    # Text area with dynamic key that changes when cleared
    text_input = st.text_area(
        label="Paste your text here:",
        height=200,
        max_chars=MAX_CHAR_LIMIT,
        placeholder="Paste the text you want to analyze here. The detector works best with at least a few sentences.",
        help=f"Maximum {MAX_CHAR_LIMIT} characters. Best results with 50+ words.",
        key=f"text_area_{st.session_state.clear_counter}"
    )
    
    # Character count
    char_count = len(text_input)
    st.caption(f"Characters: {char_count}/{MAX_CHAR_LIMIT}")
    
    # Button row with Analyze and Clear buttons
    col1, col2 = st.columns([3, 1])
    with col1:
        analyze_button = st.button("üîç Analyze Text", type="primary", use_container_width=True)
    with col2:
        clear_button = st.button("üóëÔ∏è Clear", use_container_width=True)
    
    # Handle clear button click - increment counter to force text_area recreation
    if clear_button:
        st.session_state.clear_counter += 1
        st.rerun()
    
    # Analysis and Results
    if analyze_button:
        # Validation
        if not text_input or text_input.strip() == "":
            st.error("‚ö†Ô∏è Please enter some text to analyze.")
        else:
            # Warning for very long text
            if char_count > MAX_CHAR_LIMIT:
                st.warning(f"‚ö†Ô∏è Text exceeds {MAX_CHAR_LIMIT} characters. Results may take longer.")
            
            # Warning for very short text
            word_count = len(text_input.split())
            if word_count < 10:
                st.warning("‚ö†Ô∏è Text is very short. Results may have limited confidence.")
            
            # Perform analysis
            with st.spinner("üîÑ Analyzing text..."):
                try:
                    results = analyze_text(text_input, classifier)
                    
                    # Results Section
                    st.markdown("---")
                    st.subheader("üìä Analysis Results")
                    
                    # Progress bar for AI probability
                    st.markdown("**AI Detection Level:**")
                    st.progress(results['ai_probability'] / 100)
                    
                    # Main result display
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric(
                            label="AI Probability",
                            value=f"{results['ai_probability']:.1f}%"
                        )
                    with col2:
                        st.metric(
                            label="Human Probability",
                            value=f"{results['human_probability']:.1f}%"
                        )
                    
                    # Classification result with color coding
                    st.markdown("**Classification:**")
                    classification = results['classification']
                    
                    if classification == "Human Written":
                        st.success(f"‚úÖ This text appears to be **{classification}**")
                    elif classification == "AI Generated":
                        st.error(f"ü§ñ This text appears to be **{classification}**")
                    else:
                        st.warning(f"‚ùì This text shows **{classification}** signals")
                    
                    # Confidence level
                    confidence = results['confidence_level']
                    st.markdown(f"**Confidence Level:** {confidence}")
                    
                    if confidence == "Low":
                        st.warning("‚ö†Ô∏è Low confidence result. The text may be ambiguous or the model is uncertain. "
                                 "Try with longer or more distinctive text for better results.")
                    
                    # Interpretation guidance
                    with st.expander("‚ÑπÔ∏è How to interpret these results"):
                        st.markdown("""
                        **Understanding the Results:**
                        - **AI Generated (>70% AI)**: The text likely written by ChatGPT or similar AI
                        - **Human Written (>70% Human)**: The text likely written by a human
                        - **Mixed/Uncertain**: The model cannot definitively classify the text
                        
                        **Confidence Levels:**
                        - **High (>85%)**: Very confident in the classification
                        - **Medium (70-85%)**: Reasonably confident, but some uncertainty
                        - **Low (<70%)**: Uncertain classification, interpret with caution
                        
                        **Limitations:**
                        - Best performance with English text
                        - Trained specifically on ChatGPT-generated content
                        - May not detect other AI models accurately
                        - Very short texts may yield unreliable results
                        """)
                
                except Exception as e:
                    st.error(f"‚ùå An error occurred during analysis. Please try again.")
                    st.exception(e)
    
    # Footer
    st.markdown("---")
    st.caption("üí° This tool uses machine learning and may not be 100% accurate. "
               "Use results as guidance, not definitive proof.")


if __name__ == "__main__":
    main()
